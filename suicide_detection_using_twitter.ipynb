{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chagitver/SuicideDetection/blob/main/suicide_detection_using_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80e3e9f",
      "metadata": {
        "id": "f80e3e9f"
      },
      "source": [
        "# Suicide Detection Using Twitter Data\n",
        "\n",
        "> “The bravest thing I ever did was continuing my life when I wanted to die.” – Juliette Lewis\n",
        "\n",
        "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
        "\n",
        "Suicide is a global problem. It is a permanent response to issues that are often ignored, sometimes by the individual and sometimes by the society. With social media comes a two-sided opportunity. One, people get to express themselves freely (and sometimes) with anonymity. Two, this data can sometimes be used to identify people who may be struggling, those who need help, those who are unable to ask for it directly. If this data, such as blog posts, tweets et al. can be leveraged through the power of text analytics and machine learning, we may be able to uncover frequent patterns in it. This project is a dive into the same. To see, if this theoretical possibility can be made a reality. The benefit is of course, saving a life. There is no bigger duty than to help our fellow people. That's the best we can do in life.\n",
        "\n",
        "## Contents\n",
        "- [Introduction](#introduction)\n",
        "- [Structure](#structure)\n",
        "- [Data Description](#data_description)\n",
        "- [Data Cleaning](#data_cleaning)\n",
        "- [Word Cloud](#word_cloud)\n",
        " - [All Texts](#all_text_wc)\n",
        " - [Non-Suicide Texts](#ns_wc)\n",
        " - [Suicide Texts](#s_wc)\n",
        "- [Topic Modelling](#topic_modelling)\n",
        " - [Procedure](#topic_modelling_procedure)\n",
        " - [Latent Dirichlet Allocation (LDA)](#lda)\n",
        " - [Topic Interpretation](#topic_modelling_interpretation)\n",
        "- [Making Features](#making_features)\n",
        "- [Modelling: Pre-Processing](#pre_processing)\n",
        " - [Valence Aware Dictionary and sEntiment Reasoner (VADER)](#vader)\n",
        " - [Term Frequency-Inverse Document Frequency (TF-IDF)](#tfidf)\n",
        "   - [Term Frequency (TF)](#tfidf_tf)\n",
        "   - [Inverse Document Frequency (IDF)](#tfidf_idf)\n",
        "   - [TF-IDF](#tfidf_tfidf)\n",
        "   - [TfidfVectorizer](#tfidf_vect)\n",
        " - [Adding Generated Features](#adding_generated_features)\n",
        " - [Exploratory Data Analysis](#eda)\n",
        "   - [Topic](#eda_topic)\n",
        "   - [Text Length](#eda_text_length)\n",
        "   - [Word Count](#eda_word_count)\n",
        "   - [Sentiment Score](#eda_sentiment)\n",
        " - [Clustering](#clustering)\n",
        "   - [Clustering (No PCA)](#clustering_no_pca)\n",
        "   - [Clustering (Dimensionality Reduction)](#clustering_pca)\n",
        "   - [Clustering (Comparison)](#clustering_original)\n",
        "- [Making Models](#modelling)\n",
        " - [Logistic Regression Model](#lm)\n",
        " - [Stochastic Gradient Descent Classifier](#sgd)\n",
        " - [Perceptron Classifier](#perceptron)\n",
        " - [Model Evaluation](#model_evaluation)\n",
        "- [Conclusion](#conclusion)\n",
        "\n",
        "## Structure <a class=\"anchor\" id=\"structure\"></a>\n",
        "\n",
        "The project is split into two parts. The first part will briefly look at the data, try to construct a word cloud, to get some insights such as topic modelling using LDA, and to see if the data makes any sense in and of itself. The second part is about using all we understand to build a model or classifier to identify tweets that are at risk from those which are not. Accuracy is paramount for this model and that will be our metric of evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "359ea2e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "359ea2e0",
        "outputId": "623f8a39-e2ec-4c8d-d780-a37d77a3a92d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cbc1d8ed8fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTMLParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#@title\n",
        "# Loading Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pickle\n",
        "import re\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import unidecode\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "import Stemmer\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68d22eb",
      "metadata": {
        "id": "a68d22eb"
      },
      "source": [
        "## Data Description <a class=\"anchor\" id=\"data_description\"></a>\n",
        "\n",
        "The dataset is a collection of tweets and other media from the interent. It is textual. We have freeform text, anonymously tagged with two labels `suicide` and `non-suicide`. These will be our labels for the data. As for the text, we will try to clean it and make sure the text makes logical sense.\n",
        "\n",
        "There are some discrepancies in the data. For instance, in the column that is for the labels, there is some tweet text as well. Since these rows are not too many, we will lose them. We are not losing on much of the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0115eb",
      "metadata": {
        "id": "ef0115eb"
      },
      "outputs": [],
      "source": [
        "# Loading Data\n",
        "\n",
        "data = pd.read_csv(\"suicide_detection.csv\", encoding = 'unicode_escape').iloc[:, 0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4efbde",
      "metadata": {
        "id": "7e4efbde"
      },
      "outputs": [],
      "source": [
        "data['class'].drop_duplicates().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db43f6f6",
      "metadata": {
        "id": "db43f6f6"
      },
      "outputs": [],
      "source": [
        "# Some data is in the classes. We have to remove it.\n",
        "\n",
        "data = data[data['class'].isin(['suicide', 'non-suicide'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f356cf0",
      "metadata": {
        "id": "9f356cf0"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e7a588",
      "metadata": {
        "id": "f8e7a588"
      },
      "outputs": [],
      "source": [
        "data['class'].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a510f2f",
      "metadata": {
        "id": "1a510f2f"
      },
      "source": [
        "## Data Cleaning <a class=\"anchor\" id=\"data_cleaning\"></a>\n",
        "\n",
        "To clean the data is important for any text analytics task. Open, free-form text gives users the freedom to enter what they want. This includes emoticons, punctuation, numbers, and even emoji. We will try to remove some of these from the text. Although, as a further use case beyond the scope of our exercise, we may want to keep all these features to conduct a sentiment analysis. A sentiment analysis may also help augment the data analysis. For now, we will keep it straightforward and rid our data of anything that is not text. Here is a list of the operations we are performing,\n",
        "\n",
        "- Substituting quote symbols\n",
        "- Finding all emoticons and removing them\n",
        "- Changing the text to lowercase\n",
        "- Compiling it back into a text string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0881a3b9",
      "metadata": {
        "id": "0881a3b9"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "others = ['filler', 'lt', 'gt', 'amp', 'nbsp', 'quot', 'apos', 'copy', 'reg']\n",
        "stop_words = stop_words + others\n",
        "stop_words_re = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stop_words)))\n",
        "others_re = re.compile('|'.join(map(re.escape, others)))\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    try:\n",
        "        tweet = re.sub('<[^>]*>', '', tweet)\n",
        "        emotes = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', tweet)\n",
        "        lowercase = re.sub('[\\W]+', ' ', tweet.lower())\n",
        "        no_stopwords = \" \".join(stop_words_re.sub(\"\", lowercase).split())\n",
        "        no_accents = unidecode.unidecode(no_stopwords)\n",
        "        tweet = no_accents + ' '.join(emotes).replace('-', '') \n",
        "        return tweet\n",
        "    except:\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de4c9e4",
      "metadata": {
        "id": "1de4c9e4"
      },
      "outputs": [],
      "source": [
        "clean_tweet(data.loc[1]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222de868",
      "metadata": {
        "id": "222de868"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "data['text'] = data['text'].progress_apply(clean_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5753cb",
      "metadata": {
        "id": "be5753cb"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30776830",
      "metadata": {
        "id": "30776830"
      },
      "outputs": [],
      "source": [
        "data.to_csv('cleaned_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1932722",
      "metadata": {
        "id": "a1932722"
      },
      "source": [
        "## Wordcloud <a class=\"anchor\" id=\"word_cloud\"></a>\n",
        "\n",
        "A wordcloud is well, by definition, a cloud of words. It is a common visualisation for text analysis. Essentially, we take the most important words and plot them as a cloud (or an arranged matrix) that gives us an inkling as to what the data is about. In our case, these words tell us that people are expressing themselves. They also suggest how these people may be feeling intensely and may be on the verge of commiting to a decision they cannot take back. When people have no one to talk to, they often share these heartfelt updates to millions of strangers, hoping someone will listen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905125e6",
      "metadata": {
        "id": "905125e6"
      },
      "source": [
        "### All Text <a class=\"anchor\" id=\"all_text_wc\"></a>\n",
        "\n",
        "For the first word cloud, we will use all the text and all the tweets. This would give us the overall picture of the data and tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64718aa",
      "metadata": {
        "id": "a64718aa"
      },
      "outputs": [],
      "source": [
        "all_text = ','.join(list(data.text.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe10fb14",
      "metadata": {
        "id": "fe10fb14"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(background_color=\"white\", max_words=200, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "wordcloud.generate(all_text)\n",
        "\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804e0077",
      "metadata": {
        "id": "804e0077"
      },
      "source": [
        "**Conclusion:** Here, words like `fuck`, `work`, `think`, `want`, `die`, `shit`, `parent`, `mean`, `better`, `look` are present among many others.\n",
        "\n",
        "### Non-Suicide Text <a class=\"anchor\" id=\"ns_wc\"></a>\n",
        "\n",
        "For the second word cloud, we are going to use non-suicide tweets. This can indicate if there is a significant difference between the tweets with suicide class and those with non-suicide class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be54f3c",
      "metadata": {
        "id": "5be54f3c"
      },
      "outputs": [],
      "source": [
        "non_suicide_text = \",\".join(list(data[data['class'] == 'non-suicide'].text.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee918d5c",
      "metadata": {
        "id": "ee918d5c"
      },
      "outputs": [],
      "source": [
        "ns_wordcloud = WordCloud(background_color=\"white\", max_words=200, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "ns_wordcloud.generate(non_suicide_text)\n",
        "\n",
        "ns_wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64796671",
      "metadata": {
        "id": "64796671"
      },
      "source": [
        "**Conclusion:** Here, words like `people`, `think`, `fuck`, `girl`, `life`, `time`, `friend`, `reddit`, `school` are present among many others.\n",
        "\n",
        "### Suicide Text <a class=\"anchor\" id=\"s_wc\"></a>\n",
        "\n",
        "For the third word cloud, we are going to use suicide tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1c5780",
      "metadata": {
        "id": "df1c5780"
      },
      "outputs": [],
      "source": [
        "suicide_text = \",\".join(list(data[data['class'] == 'suicide'].text.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d1867c",
      "metadata": {
        "id": "21d1867c"
      },
      "outputs": [],
      "source": [
        "s_wordcloud = WordCloud(background_color=\"white\", max_words=200, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "s_wordcloud.generate(suicide_text)\n",
        "\n",
        "s_wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bed628",
      "metadata": {
        "id": "f1bed628"
      },
      "source": [
        "**Conclusion:** Here, words like `think`, `want`, `die`, `work`, `parent`, `family`, `still` are present among many others. Interestingly, the words are quite different as shown by the word clouds. This suggests there is some inheret difference between the suicide tweets and non-suicide tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c05ebc",
      "metadata": {
        "id": "f4c05ebc"
      },
      "source": [
        "## Topic Modelling <a class=\"anchor\" id=\"topic_modelling\"></a>\n",
        "\n",
        "> \"Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents.\" ([source: monkey learn](https://monkeylearn.com/blog/introduction-to-topic-modeling/))\n",
        "\n",
        "The idea behind Topic Modelling is to take each piece of text as a `document` and try to find what `topic` this document belongs to. This is an unsupervised learning technique. We do not know what the topics are called. All we are interested in is to see whether we can find a way to categorise these documents into an arbitrary number of topics. This number is usually `5` to `10` but for large corpuses, it can go as high as required.\n",
        "\n",
        "### Procedure <a class=\"anchor\" id=\"topic_modelling_procedure\"></a>\n",
        "\n",
        "The procedure of topic modelling is simply divided into some key steps.\n",
        "\n",
        "- Tokenization or Making Words\n",
        "- Stemming & Lemmatization (removing the word extensions to only keep the root word, `formally` and `formal` both become `formal`)\n",
        "- Stopword Removal (words like `to`, `and` et al. are stop words and don't add any value to the text analysis)\n",
        "- Create a Dictionary or Bag of Words (we create a tagged dictionary of all the words in all the documents)\n",
        "- Map Corpus (we find the frequency of the words in this dictionary. For example, if `apple` has an ID of 1, and appears twice in a document, it becomes `(1,2)`)\n",
        "\n",
        "### Latent Dirichlet allocation (LDA) <a class=\"anchor\" id=\"lda\"></a>\n",
        "\n",
        "Most commonly, we use the `Latent Dirichlet allocation (LDA)` which is the method we are going to leverage as well. For LDA, we try to find the latent layer or the hidden layer to which the documents (text) belong to. For example, a book on birds and a book on reptiles belong to some logical topic. It may be `wildlife` or it may be `biology`. We don't know the label, but we know there is a connection. That is the latent layer.\n",
        "\n",
        "The LDA process is elaborated below.\n",
        "\n",
        "_Key Assumption: The key assumption of LDA is that \"each document is generated from a set of topics, and each topic was picked from a certain set of words.\"_\n",
        "\n",
        "The idea of LDA is to reverse engineer this process and find the topics from the words. This is done by the following algorithm.\n",
        "\n",
        "- Assumption: There are n topics in the corpus.\n",
        "- Distribute the n across the document m. \n",
        "- For each word w in m, assume the topic is wrong. That is, the word is misassigned.\n",
        "- Use a probability model to assign the correct topic. This is based on how many topics are in m and how many times is w given to m.\n",
        " - For example, if apple is the misassigned topic, we will assign it the correct topic based on what topics are in the document and how many other topics are assigned to apple (assumption is that the others are correct).\n",
        "- We repeat this algorithm over and over for all documents and words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06e78bdf",
      "metadata": {
        "id": "06e78bdf"
      },
      "source": [
        "![1_VTHd8nB_PBsDtd2hd87ybg.png](attachment:1_VTHd8nB_PBsDtd2hd87ybg.png \"Smoothed LDA from Wikipedia.\")\n",
        "\n",
        "_([source](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#/media/File:Smoothed_LDA.png))_\n",
        "\n",
        "Above is the LDA model where,\n",
        "\n",
        "- α is the topics-per-document\n",
        "- β is the words-per-topic\n",
        "- θ is the topic distribution for m\n",
        "- φ is the word distribution for k\n",
        "- z is the topic for word n in document m\n",
        "- w is the word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d11229",
      "metadata": {
        "id": "99d11229"
      },
      "outputs": [],
      "source": [
        "def make_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010e6ff4",
      "metadata": {
        "id": "010e6ff4"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in words if word not in stop_words] for words in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31175251",
      "metadata": {
        "id": "31175251"
      },
      "outputs": [],
      "source": [
        "words = data.text.values\n",
        "words = list(make_words(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983dc849",
      "metadata": {
        "id": "983dc849"
      },
      "outputs": [],
      "source": [
        "words = remove_stopwords(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79b1795",
      "metadata": {
        "id": "d79b1795"
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4d10f4",
      "metadata": {
        "id": "ea4d10f4"
      },
      "outputs": [],
      "source": [
        "texts = words\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus[:1][0][:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67f03f4",
      "metadata": {
        "id": "c67f03f4"
      },
      "outputs": [],
      "source": [
        "n = 6\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5060c3b",
      "metadata": {
        "id": "c5060c3b"
      },
      "outputs": [],
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba38277",
      "metadata": {
        "id": "8ba38277"
      },
      "source": [
        "### Topic Interpretation <a class=\"anchor\" id=\"topic_modelling_interpretation\"></a>\n",
        "\n",
        "We will now try to interpret the topics we've made. The above expressions show the word contributions to the topics. We have chosen to go with `6` topics but given the data is about a few things only, there may be some commonality and/or repetition.\n",
        "\n",
        "In any case, below is a description and keyword for each topic.\n",
        "\n",
        "- **Cursing/Frustration:** People who write similar tweets to this are clearly frustrated and boiling. They are angry and want to take it out with the language they use. This may be their breaking point.\n",
        "- **Seeking Help:**: Words like get, want and help suggest that the people are trying to seek help.'\n",
        "- **Hope (Unmet Hope):** The words suggest some hope which may or may not be coming to pass. Words like \"one\", \"got\", \"day\" and \"life\" appear which could be interpreted this way.\n",
        "- **Students:** Students who are succumbing to the ever-increasing pressure of school--grades, relationships, friendships, coming of age, puberty, parental pressure, substance are some things that affect students and may push them.\n",
        "- **Busy:** The word \"time\", \"want\" and \"get\" suggest that there is some conversation about not being able to make time for everything.\n",
        "- **Unclear:** This topic does not offer much comprehensive information. We will call it Unclear or Miscellaneous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a9e9a0",
      "metadata": {
        "id": "91a9e9a0"
      },
      "outputs": [],
      "source": [
        "def get_max(doc):\n",
        "    idx, l = zip(*doc)\n",
        "    return idx[np.argmax(l)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6ef508",
      "metadata": {
        "id": "1e6ef508"
      },
      "outputs": [],
      "source": [
        "topics = []\n",
        "total = len(corpus)\n",
        "\n",
        "for i in range(0, total):\n",
        "    topics.append(get_max(lda_model[corpus[i]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb547c5",
      "metadata": {
        "id": "1fb547c5"
      },
      "source": [
        "## Making Features <a class=\"anchor\" id=\"making_features\"></a>\n",
        "\n",
        "Apart from topic modelling, we can also find more features from the text. This process is called **Feature Generation**. The idea is to take the text and make numerical features out of it. Some of the features we have created are,\n",
        "\n",
        "- topic: most probable topic\n",
        "- text_length: the length of the text in characters\n",
        "- word_count: the total number of words in text, counted by spaces\n",
        "\n",
        "Other examples of this can be emoji counts, numbers and email, but all of these are currently not relevant or out of the scope of this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357232b8",
      "metadata": {
        "id": "357232b8"
      },
      "outputs": [],
      "source": [
        "data['topic'] = topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0b6f20",
      "metadata": {
        "id": "2e0b6f20"
      },
      "outputs": [],
      "source": [
        "data['text_length'] = data.text.apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0480bf70",
      "metadata": {
        "id": "0480bf70"
      },
      "outputs": [],
      "source": [
        "data['word_count'] = data.text.str.split().str.len()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2163f8",
      "metadata": {
        "id": "0d2163f8"
      },
      "source": [
        "## Modelling: Pre-Processing <a class=\"anchor\" id=\"pre_processing\"></a>\n",
        "\n",
        "A machine learning model is still a mathematic algorithm. We cannot feed it a series of words. Computers talk in numbers and that is what we need. Therefore, we have to preprocess the text into numbers. For this, we use a `vectorizer`. A vectorizer converts text into vectors or matrices. This process usually involves `Stemming` and `Tokenizing` the words as well. So, we will conduct these processes again as part of our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e7658f",
      "metadata": {
        "id": "f1e7658f"
      },
      "outputs": [],
      "source": [
        "data.text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b185466f",
      "metadata": {
        "id": "b185466f"
      },
      "outputs": [],
      "source": [
        "# Missing Check\n",
        "\n",
        "data['class'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72ca0c7b",
      "metadata": {
        "id": "72ca0c7b"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c9e0b2",
      "metadata": {
        "id": "85c9e0b2"
      },
      "source": [
        "### Valence Aware Dictionary and sEntiment Reasoner (VADER) <a class=\"anchor\" id=\"vader\"></a>\n",
        "\n",
        "VADER is a pre-trained Sentiment Analysis model in Python. It is one of the most popular ones used today. It's a parsimonious rule based model first given by [Hutto and Gilbert](https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399) in 2014. The model originally supplemented the ANEW, LICW and the General Inquirer lexicons to tune the model. Below is the model construction from the original paper.\n",
        "\n",
        "![vader-2.png](attachment:vader-2.png)\n",
        "\n",
        "_([source]())_\n",
        "\n",
        "The VADER model returns a `dict` with the `polarity_scores()` function. The dict has proportions named `pos`, `neu`, `neg` but it also returns a `compound` variable that is a score in the range of `[-1, 1]`. We are going to leverage this very score for our model feature augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6cb0d6",
      "metadata": {
        "id": "9b6cb0d6"
      },
      "outputs": [],
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    return float(vader.polarity_scores(text)['compound'])\n",
        "\n",
        "data['sentiment'] = data['text'].progress_apply(get_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f109b48",
      "metadata": {
        "id": "4f109b48"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef68e108",
      "metadata": {
        "id": "ef68e108"
      },
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF) <a class=\"anchor\" id=\"tfidf\"></a>\n",
        "\n",
        "A note on `TF-IDF` that we have used as vectorizer procedure is important here. Term Frequency - Inverse Document Frequency or TF-IDF is a feature generation and information retrieval (IR) technique which can quantify how relevant a string or a bigram (collection of two words) is to the document. Ideally, when we vectorize it in this process, we are creating a measure of the most important strings from all of the possible string combinations. This creates sparse matrices. That is, matrices with zeros for each of the documents (tweets, in this case).\n",
        "\n",
        "TF-IDF is the measure of relative frequency of the words. The process is breifly explained below.\n",
        "\n",
        "#### Term Frequency (TF) <a class=\"anchor\" id=\"tfidf_tf\"></a>\n",
        "\n",
        "The term frequency is simply the number of occurences of a word (term) versus the total occurrences of all words in the document. This is the TF.\n",
        "\n",
        "${tf}_{i,j} = \\tfrac{ {n_{i,j}} } { \\sum_{k} {n_{i,j}} } $\n",
        "\n",
        "#### Inverse Document Frequency (IDF) <a class=\"anchor\" id=\"tfidf_idf\"></a>\n",
        "\n",
        "For the IDF, we calculate the `log` of number of documents by the number of documents that contain the word w. This tells us the weight of rare words in the document.\n",
        "\n",
        "$ {idf (w) = log(\\tfrac{N}{df_{t}} ) } $\n",
        "\n",
        "#### TF-IDF <a class=\"anchor\" id=\"tfidf_tfidf\"></a>\n",
        "\n",
        "The TF-IDF is the product of the above two. This gives us a measure of the word relevancy or importance.\n",
        "\n",
        "$ {w}_{i,j} = \\tfrac{ {n_{i,j}} } { \\sum_{k} {n_{i,j}} } \\cdot { log(\\tfrac{N}{df_{t}} ) } $\n",
        "\n",
        "\n",
        "#### TfidfVectorizer <a class=\"anchor\" id=\"tfidf_vect\"></a>\n",
        "\n",
        "Instead of using TF-IDF explicitly, the `sklearn TfidfVectorizer` does all the work for us!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e4859b",
      "metadata": {
        "id": "63e4859b"
      },
      "outputs": [],
      "source": [
        "english_stemmer = Stemmer.Stemmer('en')\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
        "        return lambda doc: english_stemmer.stemWords(analyzer(doc))\n",
        "tfidf = StemmedTfidfVectorizer(min_df=1, stop_words='english', analyzer='word', ngram_range=(1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486be6d8",
      "metadata": {
        "id": "486be6d8"
      },
      "outputs": [],
      "source": [
        "X = tfidf.fit_transform(data.text.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b04b67",
      "metadata": {
        "id": "69b04b67"
      },
      "source": [
        "### Adding Generated Features <a class=\"anchor\" id=\"adding_generated_features\"></a>\n",
        "\n",
        "Now that we have the matrices, we add the information we extracted to the matrices. This ensures our analysis and topic modelling feeds into the model as well. This may be the reason it performs well, if it does. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaaf8497",
      "metadata": {
        "id": "aaaf8497"
      },
      "source": [
        "##### Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9fe7b8",
      "metadata": {
        "id": "bd9fe7b8"
      },
      "outputs": [],
      "source": [
        "X = hstack((X, np.array(data['topic'])[:,None])).tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d5eabb",
      "metadata": {
        "id": "a3d5eabb"
      },
      "source": [
        "##### Text Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97290665",
      "metadata": {
        "id": "97290665"
      },
      "outputs": [],
      "source": [
        "X = hstack((X, np.array(data['text_length'])[:,None])).tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd03302b",
      "metadata": {
        "id": "dd03302b"
      },
      "source": [
        "##### Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0632cc7",
      "metadata": {
        "id": "c0632cc7"
      },
      "outputs": [],
      "source": [
        "X = hstack((X, np.array(data['word_count'])[:,None])).tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629c8500",
      "metadata": {
        "id": "629c8500"
      },
      "source": [
        "##### Sentiment (VADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7d8204",
      "metadata": {
        "id": "7d7d8204"
      },
      "outputs": [],
      "source": [
        "X = hstack((X, np.array(data['sentiment'])[:,None])).tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7f9aea",
      "metadata": {
        "id": "ef7f9aea"
      },
      "source": [
        "#### Converting Y to Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c796ac93",
      "metadata": {
        "id": "c796ac93"
      },
      "outputs": [],
      "source": [
        "y = data['class'].replace(['non-suicide', 'suicide'],[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569c9c43",
      "metadata": {
        "id": "569c9c43"
      },
      "source": [
        "## Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"></a>\n",
        "\n",
        "For the features we have just calculated, we can conduct EDA which is to say we will plot them and view them individually to see if there are certain patterns in them. Since most of the features are extracted, we will only use them as they are the only numerical features available.\n",
        "\n",
        "- Topic\n",
        "- Text Length\n",
        "- Word Count\n",
        "- Sentiment\n",
        "\n",
        "### Topic <a class=\"anchor\" id=\"eda_topic\"></a>\n",
        "\n",
        "While we've made LDA topics, it would be curious to see their distribution and if there is some sort of imbalance in the data when it comes to topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2deb35",
      "metadata": {
        "id": "2b2deb35"
      },
      "outputs": [],
      "source": [
        "topic_summary = data.groupby('topic').agg('count').reset_index()\n",
        "topic_summary = topic_summary[['topic', 'Unnamed: 0']]\n",
        "topic_summary.rename(columns={'Unnamed: 0': 'count'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aada0b9",
      "metadata": {
        "id": "7aada0b9"
      },
      "outputs": [],
      "source": [
        "topic_summary['topic'] = topic_summary.topic.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601e0c98",
      "metadata": {
        "id": "601e0c98"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x = 'topic', y = 'count', data = topic_summary, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790c3ff9",
      "metadata": {
        "id": "790c3ff9"
      },
      "source": [
        "**Interpretation:** As we can see, topic 1 (Seeking Help) has the highest representation in the data, followed by topic 4 (Busy) and 2 (Hope - Unmet Hope). The lowest representation comes from topic 5 (Unclear). Overall, this tells us that out of the 6 topics that we made, the distribution is not normal or balanced. Also, since the least amount of representation is Unclear, we can say we have identified topics for most of the people involved. This is good because that means we are able to successfully understand the topics or contexts people have tweeted about. It is also interesting how **Seeking Help** is the most common topic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b60ba6b",
      "metadata": {
        "id": "8b60ba6b"
      },
      "source": [
        "### Text Length <a class=\"anchor\" id=\"eda_text_length\"></a>\n",
        "\n",
        "The text length variable is a univariate variable since there is only the numeric length of text in characters. We will make a histogram for this variable and see if the text lengths are distributed normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bd84b4",
      "metadata": {
        "id": "72bd84b4"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data['text_length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3479d45",
      "metadata": {
        "id": "c3479d45"
      },
      "source": [
        "**Interpretation:** From the histogram, we can see that the range seems to be too high. That suggests there are outliers, but we can also see that most of the text is short and it falls slowly towards longer ranges. This suggests the data is left-skewed, which is usual for online texts and is a bit obvious given the nature of social media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a215c45",
      "metadata": {
        "id": "2a215c45"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data['text_length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1b4387",
      "metadata": {
        "id": "7e1b4387"
      },
      "source": [
        "**Interpretation:** With the boxplot, we see a similar pattern. There are many outliers in the data that increase the range of the data, which suggests that the text is quite varied in length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08030fcd",
      "metadata": {
        "id": "08030fcd"
      },
      "outputs": [],
      "source": [
        "data.text_length.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11bef01b",
      "metadata": {
        "id": "11bef01b"
      },
      "source": [
        "**Interpretation:** As we agreed above with the plots, the summary says the average length is 403 charaacters while the median length is 189 characters. Also, the maximum is 33441 which suggests the outlier situation. From the 75th quantile to the 100% quantile, the leap is quite long. (467 -> 33441)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f92b56",
      "metadata": {
        "id": "f9f92b56"
      },
      "source": [
        "### Word Count <a class=\"anchor\" id=\"eda_word_count\"></a>\n",
        "\n",
        "Similar to text length, we will look at the word count as a histogram, as a boxplot and as a numeric summary to understand the distribution and if there is any skewness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4173722b",
      "metadata": {
        "id": "4173722b"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data['word_count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab284fe9",
      "metadata": {
        "id": "ab284fe9"
      },
      "source": [
        "**Interpretation:** Just as in the case with `text_length`, the `word_count` variable is distributed with a left-skewness. That is not a surprise since there is bound to be some correlation between the two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d2ed75",
      "metadata": {
        "id": "32d2ed75"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data['word_count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3816cd3",
      "metadata": {
        "id": "f3816cd3"
      },
      "source": [
        "**Interpretation:** There are outliers but the major ranges are concentrated towards the left, further cementing the left-skewness of the data. The findings are in agreement with text length as well as the general nature of social media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e95e4a2",
      "metadata": {
        "id": "6e95e4a2"
      },
      "outputs": [],
      "source": [
        "data.word_count.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1d917b",
      "metadata": {
        "id": "ae1d917b"
      },
      "source": [
        "**Interpretation:** If we notice, there is a fantastic leap between the 75th quantile and the 100th quantile. 74 to 15266. This suggests that most of the data has less than 74 words. It is only 25% of the data that goes beyond it till 15266."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1459872b",
      "metadata": {
        "id": "1459872b"
      },
      "source": [
        "### Sentiment Score <a class=\"anchor\" id=\"eda_sentiment\"></a>\n",
        "\n",
        "The sentiment score is interesting since it is a numeric variable but we will have to create a summary for it to understand the sentiments and their distributions. Overall, we can also view it as a fully numeric variable so we will also create univariate plots and we will also look at it as a bivariate data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0fe7ea",
      "metadata": {
        "id": "9c0fe7ea"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data.sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728ba7d7",
      "metadata": {
        "id": "728ba7d7"
      },
      "source": [
        "**Interpretation:** Naturally, the data has a long distribution towards `-1` which is fully negative. The lowest peak is at `+1` but there are a lot of neutral `0` text entries as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fde48e8",
      "metadata": {
        "id": "6fde48e8"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data.sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0347fcb",
      "metadata": {
        "id": "c0347fcb"
      },
      "source": [
        "**Interpretation:** The median sentiments score is slightly left of 0, which suggests that there is an overall negative connotation to the data. This makes sense given the nature and context of the analysis. In fact, it is fortunate that there are any neutral and positive connotations at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0b037e",
      "metadata": {
        "id": "3d0b037e"
      },
      "outputs": [],
      "source": [
        "data.sentiment.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff25aab",
      "metadata": {
        "id": "7ff25aab"
      },
      "outputs": [],
      "source": [
        "sentiment_summary = pd.DataFrame(data['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af32895b",
      "metadata": {
        "id": "af32895b"
      },
      "outputs": [],
      "source": [
        "sentiment_summary.loc[sentiment_summary.sentiment == 0, 'connotation'] = 'neutral'\n",
        "sentiment_summary.loc[sentiment_summary.sentiment < 0, 'connotation'] = 'negative'\n",
        "sentiment_summary.loc[sentiment_summary.sentiment > 0, 'connotation'] = 'positive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f1f3ba",
      "metadata": {
        "id": "11f1f3ba"
      },
      "outputs": [],
      "source": [
        "sentiment_summary = sentiment_summary.groupby('connotation').agg(['count', 'mean']).reset_index().droplevel(level=0, axis=1)\n",
        "sentiment_summary.reset_index()\n",
        "columns = ['connotation', 'count', 'mean']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913b991a",
      "metadata": {
        "id": "913b991a"
      },
      "outputs": [],
      "source": [
        "sentiment_summary.rename( columns={i:j for i,j in zip(sentiment_summary.columns.tolist(),columns)}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0bd158",
      "metadata": {
        "id": "8d0bd158"
      },
      "outputs": [],
      "source": [
        "sentiment_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9e6291",
      "metadata": {
        "id": "0c9e6291"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='connotation', y='count', data = sentiment_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fc73c7",
      "metadata": {
        "id": "f2fc73c7"
      },
      "source": [
        "**Interpretation**: As we can see, the maximum number of text entries are negative, some are positive and the lowest are neutral. In terms of counts, the counts are 1,20,834 for negative, 97,325 for positive and 13,886 for neutral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83c71de",
      "metadata": {
        "id": "d83c71de"
      },
      "outputs": [],
      "source": [
        "sentiment_summary['abs_mean'] = np.abs(sentiment_summary['mean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2deecd49",
      "metadata": {
        "id": "2deecd49"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='connotation', y='abs_mean', data = sentiment_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c389557",
      "metadata": {
        "id": "9c389557"
      },
      "source": [
        "**Interpretation**: For this plot, we are looking at the absolute value of the mean for all three labels. For neutral, the mean will be 0 of course. The goal is to check the valence of the emotion or sentiment. It is an analysis of \"how much?\". In general, people who have negative tweets are more negative than those who have positive tweets are positive. This indicates that the negative connotation is higher in a magnitudal or valence sense as well.\n",
        "\n",
        "This concludes our EDA, we will now proceed to conducting a clustering exercise to see if there is a natural two class division in the data. For this, we will assume it to be unlabelled data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13b8ce9",
      "metadata": {
        "id": "d13b8ce9"
      },
      "source": [
        "## Clustering <a class=\"anchor\" id=\"clustering\"></a>\n",
        "\n",
        "Clustering is the process of taking unsupervised or unlabelled data and finding common properties in them. The idea is to split the data into groups or clusters. Here, we will conduct clustering in two ways. The first will be with the high dimensional data. The second will be by doing Principal Component Analysis (PCA) and reducing this dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d8da56",
      "metadata": {
        "id": "87d8da56"
      },
      "source": [
        "### Clustering (No PCA) <a class=\"anchor\" id=\"clustering_no_pca\"></a>\n",
        "\n",
        "For this, we will directly cluster the data into two segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac986c3",
      "metadata": {
        "id": "7ac986c3"
      },
      "outputs": [],
      "source": [
        "kmeans_vanilla = KMeans(n_clusters=2, random_state=42).fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234f713f",
      "metadata": {
        "id": "234f713f"
      },
      "outputs": [],
      "source": [
        "kmeans_vanilla_labels = kmeans_vanilla.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5831a6",
      "metadata": {
        "id": "5d5831a6"
      },
      "source": [
        "**Limitation:** The sparse matrices are too large to be plot into a scatter graph. The required Memory exceeds what we have at hand currently. In any case, we have displayed the data points as a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6d57fa",
      "metadata": {
        "id": "7c6d57fa"
      },
      "outputs": [],
      "source": [
        "cluster_vanilla = pd.DataFrame(kmeans_vanilla_labels)\n",
        "cluster_vanilla.columns = ['cluster']\n",
        "cluster_vanilla['cluster'] = cluster_vanilla.cluster.astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73d04e1",
      "metadata": {
        "id": "e73d04e1"
      },
      "outputs": [],
      "source": [
        "cluster_vanilla = pd.DataFrame(cluster_vanilla.cluster.value_counts()).reset_index()\n",
        "cluster_vanilla.columns = ['cluster', 'count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5272d4ca",
      "metadata": {
        "id": "5272d4ca"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x = 'cluster', y = 'count', data=cluster_vanilla)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854099f0",
      "metadata": {
        "id": "854099f0"
      },
      "source": [
        "**Interpretation:** When we cluster without doing a PCA, we end up with really disproportionate clusters. This seems to suggest that the data is not very well divided into two categories naturally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0857f63",
      "metadata": {
        "id": "b0857f63"
      },
      "source": [
        "### Clustering (Dimensionality Reduction) <a class=\"anchor\" id=\"clustering_pca\"></a>\n",
        "\n",
        "For this, we will first conduct the PCA process and then cluster the data into two segments.\n",
        "\n",
        "**Note:** Since our matrix is sparse, we are going to use the TruncatedSVD for dimensionality reduction and not PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90364d44",
      "metadata": {
        "id": "90364d44"
      },
      "outputs": [],
      "source": [
        "tsvd = TruncatedSVD(n_components = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d25538a",
      "metadata": {
        "id": "9d25538a"
      },
      "outputs": [],
      "source": [
        "svd_i = []\n",
        "svd_variance = []\n",
        "\n",
        "for i in tqdm(range(0,20)):\n",
        "    svd_i.append(i)\n",
        "    tsvd = TruncatedSVD(n_components = i)\n",
        "    tsvd.fit(X)\n",
        "    svd_variance.append(tsvd.explained_variance_ratio_.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba47d65d",
      "metadata": {
        "id": "ba47d65d"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=svd_i, y=svd_variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511ce123",
      "metadata": {
        "id": "511ce123"
      },
      "outputs": [],
      "source": [
        "dict(zip(svd_i, svd_variance))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a241b949",
      "metadata": {
        "id": "a241b949"
      },
      "source": [
        "**Interpretation:** The plot suggests that even one component almost fully explains the varaiance in X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2685b105",
      "metadata": {
        "id": "2685b105"
      },
      "outputs": [],
      "source": [
        "X_tsvd = TruncatedSVD(n_components = 1).fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf06313",
      "metadata": {
        "id": "0bf06313"
      },
      "outputs": [],
      "source": [
        "kmeans_reduced = KMeans(n_clusters=2, random_state=42).fit(X_tsvd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162f85e7",
      "metadata": {
        "id": "162f85e7"
      },
      "outputs": [],
      "source": [
        "kmeans_reduced_labels = kmeans_reduced.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca17647",
      "metadata": {
        "id": "7ca17647"
      },
      "source": [
        "**Limitation:** Again, the sparse matrices are too large to be plot into a graph. The required Memory exceeds what we have at hand currently. In any case, we have displayed the data points as a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff370318",
      "metadata": {
        "id": "ff370318"
      },
      "outputs": [],
      "source": [
        "cluster_reduced = pd.DataFrame(kmeans_reduced_labels)\n",
        "cluster_reduced.columns = ['cluster']\n",
        "cluster_reduced['cluster'] = cluster_reduced.cluster.astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33694428",
      "metadata": {
        "id": "33694428"
      },
      "outputs": [],
      "source": [
        "cluster_reduced = pd.DataFrame(cluster_reduced.cluster.value_counts()).reset_index()\n",
        "cluster_reduced.columns = ['cluster', 'count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d57d95c",
      "metadata": {
        "id": "0d57d95c"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x = 'cluster', y = 'count', data=cluster_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71058b2d",
      "metadata": {
        "id": "71058b2d"
      },
      "source": [
        "**Interpretation:** The 0 and 1 are not relevant. Essentially, as per the counts, it is evident that the data can be clustered into two very uneven groups if taken as unlabelled. But without being able to look at the plot, we **cannot** say for certain if the clusters are not divided. The memory issue limitation (we need about 236 GB of RAM to make the sparse matrix into a matrix) makes that impossible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8c85bd",
      "metadata": {
        "id": "aa8c85bd"
      },
      "source": [
        "### Clustering (Comparison) <a class=\"anchor\" id=\"clustering_original\"></a>\n",
        "\n",
        "One little attempt at this can be to try and plot the original classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8ba684",
      "metadata": {
        "id": "8c8ba684"
      },
      "outputs": [],
      "source": [
        "class_data = pd.DataFrame(data['class'].value_counts()).reset_index()\n",
        "class_data.columns = ['label', 'count']\n",
        "sns.barplot(x='label', y='count', data=class_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc0c39d",
      "metadata": {
        "id": "ffc0c39d"
      },
      "source": [
        "**Interpretation:** Interestingly, the original class data is balanced. The clustering does not indicate much confidence over whether the data is truly dividable but we tread on and we will try to make models to try and discriminate the data based on our features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19cdc5e4",
      "metadata": {
        "id": "19cdc5e4"
      },
      "source": [
        "## Making Models <a class=\"anchor\" id=\"modelling\"></a>\n",
        "\n",
        "To make the model, we are going to split the data into a `75`:`25`::`train`:`test` ratio. Then, we will use the following models,\n",
        "\n",
        "- Logistic Regression Model\n",
        "- Stochastic Gradient Descent Classifier\n",
        "- Perceptron\n",
        "\n",
        "to separate between the two categories (now encoded as labels). Hence, the model returns a 1 or a 0. That tells us what the response is. The training process is straightforward. Most of the heavy lifting is done as part of Feature Generation, Information Retrieval and Pre-Processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a38de93",
      "metadata": {
        "id": "6a38de93"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45116184",
      "metadata": {
        "id": "45116184"
      },
      "source": [
        "## Logistic Regression Model <a class=\"anchor\" id=\"lm\"></a>\n",
        "\n",
        "A logistic regression model works to calculate the log-odds. It is essentally a linear model that finds odds/probabilities of something being a certain label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37a94e8",
      "metadata": {
        "id": "e37a94e8"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(random_state = 42)\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77c3daa",
      "metadata": {
        "id": "e77c3daa"
      },
      "outputs": [],
      "source": [
        "lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec7611d",
      "metadata": {
        "id": "8ec7611d"
      },
      "outputs": [],
      "source": [
        "lr_train = round(lr.score(X_train, y_train)*100, 2)\n",
        "print(f'Logistic Regression Train Accuracy: {lr_train}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a0ab83",
      "metadata": {
        "id": "93a0ab83"
      },
      "outputs": [],
      "source": [
        "lr_test = round(lr.score(X_test, y_test)*100, 2)\n",
        "print(f'Logistic Regression Test Accuracy: {lr_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81749f8",
      "metadata": {
        "id": "c81749f8"
      },
      "source": [
        "## SGD Classifier <a class=\"anchor\" id=\"sgd\"></a>\n",
        "\n",
        "The SGD Classifier performs linear learning using models like SVM, LM et al. but adds the Stochastic Gradient Descent to their process. This works with data in sparse arrays of floating point values for the features. That is why this model is chosen -- our data is sparse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba49c25a",
      "metadata": {
        "id": "ba49c25a"
      },
      "outputs": [],
      "source": [
        "sgd = SGDClassifier(random_state = 42)\n",
        "sgd.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a10070f7",
      "metadata": {
        "id": "a10070f7"
      },
      "outputs": [],
      "source": [
        "sgd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530751cb",
      "metadata": {
        "id": "530751cb"
      },
      "outputs": [],
      "source": [
        "sgd_train = round(sgd.score(X_train, y_train)*100, 2)\n",
        "print(f'Stochastic Gradient Descent Classifier Train Accuracy: {sgd_train}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75e31ba",
      "metadata": {
        "scrolled": true,
        "id": "a75e31ba"
      },
      "outputs": [],
      "source": [
        "sgd_test = round(sgd.score(X_test, y_test)*100, 2)\n",
        "print(f'Stochastic Gradient Descent Classifier Test Accuracy: {sgd_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0ea914",
      "metadata": {
        "id": "cd0ea914"
      },
      "source": [
        "## Perceptron <a class=\"anchor\" id=\"perceptron\"></a>\n",
        "\n",
        "This is a simple linear perceptron classifier. The idea to use this is because if it attains the same accuracy as the others, we might be able to conclude this as a baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2810e0",
      "metadata": {
        "id": "3b2810e0"
      },
      "outputs": [],
      "source": [
        "p = Perceptron(random_state = 42)\n",
        "p.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b598b5",
      "metadata": {
        "id": "18b598b5"
      },
      "outputs": [],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec6c657",
      "metadata": {
        "id": "1ec6c657"
      },
      "outputs": [],
      "source": [
        "p_train = round(p.score(X_train, y_train)*100, 2)\n",
        "print(f'Perceptron Classifier Train Accuracy: {p_train}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d435de81",
      "metadata": {
        "scrolled": true,
        "id": "d435de81"
      },
      "outputs": [],
      "source": [
        "p_test = round(p.score(X_test, y_test)*100, 2)\n",
        "print(f'Perceptron Classifier Test Accuracy: {p_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe00cfc6",
      "metadata": {
        "id": "fe00cfc6"
      },
      "source": [
        "### Model Evaluation <a class=\"anchor\" id=\"model_evaluation\"></a>\n",
        "\n",
        "To evaluate the model, we are only interested in the accuracy at the time. If we look at the table, we can see that out of all three, the Logistic Regression performs the best, followed by the SGD. The Perceptron being a simpler model overfits in training and out of the three test accuracies, performs the worst.\n",
        "\n",
        "The SGD is essentially a linear SVM which may be too complex a model for this data. Our matrix is sparse, however, which makes it ideal for the SGD. Perhaps, with some hyperparameter tuning the SGD may outperform the logistic regression, but for now, the LR is the most ideal model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f30bcf5e",
      "metadata": {
        "id": "f30bcf5e"
      },
      "outputs": [],
      "source": [
        "# Combining\n",
        "\n",
        "models = ['LR', 'SGD', 'Perceptron']\n",
        "trains = [lr_train, sgd_train, p_train]\n",
        "tests = [lr_test, sgd_test, p_test]\n",
        "\n",
        "pd.DataFrame(list(zip(models, trains, tests)),\n",
        "              columns=['model','train_accuracy', 'test_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a595d969",
      "metadata": {
        "id": "a595d969"
      },
      "source": [
        "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
        "\n",
        "As a summary, we first created word clouds for all words, suicide tweets and non-suicide tweets. This indicated whether there is a difference in behaviour, verbage etc for these categories. Then, we conducted topic modelling on the text and made and inferred six topics. This data was augmented along with other data into the dataset. Then, we applied sentiment analysis using VADER and used that score as a column in the data. Finally, we conducted TF-IDF and got the final sparse matrix. All of this was combined into a final dataset to make it ready for the model. Before modelling, we conducted EDA and viewed the variables. lso, we clustered the data with and without dimensionality reduction then compared it with the original labels. Finally, we used three different models, after splitting the data, and checked which model generalises the best. It seems a Logistic Regression model outperforms the other two options.\n",
        "\n",
        "Does this solve the problem of suicides? Of course not. But it provides a glimpse at the possibility. It provides us a step to move in the right direction. Today, we create data faster than ever before. In fact, in one day we create data that was not created in thousands of years. This data has a potential to make large changes and improve systems, of course, but it also has the potential to save lives. This project is but a humble attempt to show the same. Social Media platforms, for example, can attach check-ins when their models detect some behaviour that seems unsafe or hints towards suicidal ideation and tendencies.\n",
        "\n",
        "In any case, this concludes the report."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}